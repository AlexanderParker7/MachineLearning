{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with Python: Theory and Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Naturally-learned-ability-for-problem-solving\" data-toc-modified-id=\"Naturally-learned-ability-for-problem-solving-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Naturally learned ability for problem solving</a></span></li><li><span><a href=\"#Physics-law-based-models\" data-toc-modified-id=\"Physics-law-based-models-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Physics-law-based models</a></span></li><li><span><a href=\"#Machine-learning-models,-data-based\" data-toc-modified-id=\"Machine-learning-models,-data-based-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Machine learning models, data-based</a></span></li><li><span><a href=\"#General-steps-for-training-machine-learning-models\" data-toc-modified-id=\"General-steps-for-training-machine-learning-models-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>General steps for training machine learning models</a></span></li><li><span><a href=\"#Some-mathematical-concepts,-variables,-and-spaces\" data-toc-modified-id=\"Some-mathematical-concepts,-variables,-and-spaces-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Some mathematical concepts, variables, and spaces</a></span><ul class=\"toc-item\"><li><span><a href=\"#Toy-examples\" data-toc-modified-id=\"Toy-examples-1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Toy examples</a></span></li><li><span><a href=\"#Feature-space\" data-toc-modified-id=\"Feature-space-1.5.2\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>Feature space</a></span></li><li><span><a href=\"#Affine-space\" data-toc-modified-id=\"Affine-space-1.5.3\"><span class=\"toc-item-num\">1.5.3&nbsp;&nbsp;</span>Affine space</a></span></li><li><span><a href=\"#Label-space\" data-toc-modified-id=\"Label-space-1.5.4\"><span class=\"toc-item-num\">1.5.4&nbsp;&nbsp;</span>Label space</a></span></li><li><span><a href=\"#Hypothesis-space\" data-toc-modified-id=\"Hypothesis-space-1.5.5\"><span class=\"toc-item-num\">1.5.5&nbsp;&nbsp;</span>Hypothesis space</a></span></li><li><span><a href=\"#Definition-of-a-typical-machine-learning-model,-a-mathematical-view\" data-toc-modified-id=\"Definition-of-a-typical-machine-learning-model,-a-mathematical-view-1.5.6\"><span class=\"toc-item-num\">1.5.6&nbsp;&nbsp;</span>Definition of a typical machine learning model, a mathematical view</a></span></li></ul></li><li><span><a href=\"#Requirements-for-creating-machine-learning-models\" data-toc-modified-id=\"Requirements-for-creating-machine-learning-models-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Requirements for creating machine learning models</a></span></li><li><span><a href=\"#Types-of-data\" data-toc-modified-id=\"Types-of-data-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Types of data</a></span></li><li><span><a href=\"#Relation-between-physics-law-based-and-data-based-models\" data-toc-modified-id=\"Relation-between-physics-law-based-and-data-based-models-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Relation between physics-law-based and data-based models</a></span></li><li><span><a href=\"#This-book\" data-toc-modified-id=\"This-book-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>This book</a></span></li><li><span><a href=\"#Who-may-read-this-book\" data-toc-modified-id=\"Who-may-read-this-book-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>Who may read this book</a></span></li><li><span><a href=\"#Codes-used-in-this-book\" data-toc-modified-id=\"Codes-used-in-this-book-1.11\"><span class=\"toc-item-num\">1.11&nbsp;&nbsp;</span>Codes used in this book</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naturally learned ability for problem solving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We are constantly dealing with all kinds of problems every day, and would like to solve these problems for timely decisions  and actions. We may notice that to many of the daily-life problems, our decisions are often made spontaneously, swift without much consciousness. This is because we have been constantly learning to solve such problems in the past since we were born, and therefore the solutions have already been encoded in the neuron cells in our brain. When facing similar problems, our decision is spontaneous. \n",
    "\n",
    "For many complicated problems, especially in science and engineering, one would need to think harder and even conduct extensive research and study on the related issues before we can provide a solution. What if we want to give spontaneous reliable solutions also to these types of problems? Some scientists and engineers may be able to do this for some problems, but not many. Those scientists are intensively trained or educated in specially designed courses for dealing with complicated problems. \n",
    "\n",
    "What if a normal layman would also like to be able solve these challenging types of problem? One way is to go through a special learning process. The alternative may be through machine learning, to develop a special computer model with a mechanism that can be trained to extract features from the experience or data  to provide a reliable and instantaneous solution for a type of problems.\n",
    "\n",
    "${}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physics-law-based models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems in science and engineering are usually much more difficult to solve. This is because we humans can only experience or observe the phenomenon associated with the problem. However, many phenomena are not easily observable and have very complicated underline logics. Solving these types of problems by human via natural learning may not be the most effective approach. Instead, scientists have been trying to unveil these underline logics by developing some theories (or laws or principles) that can help to best describe these phenomena. These theories are then formulated in the form of algebraic or differential or integral system equations that govern the key variables involved in the phenomena. The next step is then to find a method that can solve these equations for these variables varying in space and with time. The final step is to find a way to validate the theory, by observation and/or experiments to measure the values of these variables. The validated theory is used to build models to solve problems that exhibits the same phenomena.This type of models is called physics-law-based models.  \n",
    "\n",
    "The above-mentioned process is essentially what humans on earth have been doing in trying to understand the nature, and we have made tremendous progress so far.  In this process, we have established a huge number of areas of studies: physics, mathematics, biology, etc., which are  now referred as sciences. \n",
    "\n",
    "Understanding the nature is only a part of the story. Humans want to invent and build new things. Good understanding on various phenomena enables us to do so, and we have practically built everything around us, buildings, bridges, airplanes, space stations, cars, ships, computers, cell phones, internet, communication systems, and energy systems. Such a list is endless. In this process, we humans established a huge number of areas of development, which we are now referred as engineering. \n",
    "\n",
    "Understanding biology helped us to discover medicines, treatments for illnesses of humans and animals, treatments for plants and the environment, as well as proper measure and policies dealing with the relationship between humans, animals, plants, and environments. In this process, we humans established a huge number of areas of studies, including medicine, agriculture, ecology, etc.\n",
    "\n",
    "In the relentless quests by humans in the history, countless theories, laws, techniques, methods, etc. have been developed in various areas of science, engineering, and biology. For example, in the study of a small area of computational mechanics for design of structural systems, we have developed the finite element methods (FEM) \\cite{liu2013finite}, smoothed finite element methods (S-FEM) \\cite{liusmoothed}, meshfree methods \\cite{liu2002mesh, liu2013smoothed}, inverse techniques \\cite{liu592computational}, etc., just to name a few that the author has been working on. It is not possible and necessary to list all of these kinds of methods and techniques. Our discussion here is just to provide an overall view on how a problem can be solved based on physics laws. \n",
    "\n",
    "Note that there are many problems in nature, engineering, and society for which it is difficult to describe find proper physics laws to accurately and effectively. Alternative means are thus needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning models, data-based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a large class of complicated problems (in science, engineering, biology, and daily-life) that do not yet have known governing physics laws, or the solutions to the governing laws equations are too expensive to obtain. For this type of problems, on the other hand, we often have some data obtained and accumulated through observations or measurements or historic records. When the data is sufficiently large and of good quality, it is possible to develop computer models to learn from these data. Such a model can then be used to find a solution for this type of problems. This kind of computer models is defined as data-based models or machine learning models in this book. \n",
    "\n",
    "Types of effective artificial neural networks (ANNs) with various configurations have been developed and widely used for practical problems in sciences and engineering, including multilayer perceptron (MLP) \\cite{rosenblatt1962, rumelhart:errorprop, liu2019trumpet, liu2021tubenet},  Convolutional Neural Networks (CNNs) \\cite{Fukushima1980, multi-column2012, VALUEVA2020232, duanLawnMower2021, duan2021anchor}, Recurrent Neural Networks (RNNs) \\cite{mcculloch43a, Schmidhuber1993, Yu2019}, among others. The TrumpetNets \\cite{liu2019trumpet} and TubeNets \\cite{liu2021tubenet, shi2020tubenet, duanUncertaintyInverse2way2021, duanJointStiff2021} were also recently proposed by the author for creating two-way deepnets using physics-law-based models as trainers, such as the FEM \\cite{liu2013finite} and S-FEM \\cite{liusmoothed}. The unique feature of the TrumpetNets and the TubeNets is the effectiveness for both forward and inverse problems \\cite{liu592computational}, by design unique net architecture. Most importantly, solutions to inverse problems can be analytically derived in explicit formulae for the first time.\n",
    "\n",
    "Machine learning is essentially to mimic the natural learning process occurring in biological brains that can have a huge number of neurons. In terms of usage of data, we may have **three major categories**:  \n",
    "\n",
    "1. Supervised Learning, using data with true labels (teachers)\n",
    "1. Unsupervised Learning, using data without labels\n",
    "1. Reinforcement Learning, using a predefined environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of **types of problems** to solve, there are \n",
    "\n",
    "1. Binary classification problems, answer in probability to yes or no \n",
    "2. k-classification problems, answer in probabilities to k classes\n",
    "3. k-clustering problems, answer in k clusters of data-points\n",
    "4. Regression (linear or nonlinear), answer in predictions of continuous functions\n",
    "5. Feature extraction, answer in key features in the dataset\n",
    "6. Abnormality detection, answer in abnormal data\n",
    "7. Inverse analysis, answer in prediction on features from known responses\n",
    "\n",
    "In terms of **learning methodology** or algorithms, we have\n",
    "\n",
    "1. Linear and logistic regression, supervised \n",
    "2. Decision Tree, supervised\n",
    "3. Support Vector Machine (SVM), supervised\n",
    "4. Naive Bayes, supervised\n",
    "5. Multi-Layer Perceptron (MLP) or artificial Neural Networks (NNs), supervised\n",
    "6. k-Nearest Neighbors (kNN), supervised\n",
    "7. Random Forest, supervised\n",
    "8. Gradient Boosting types of algorithms, supervised\n",
    "9. Principal Components Analysis (PCA), unsupervised\n",
    "10. K-Means, Mean-Shift, unsupervised\n",
    "11. Autoencoders, unsupervised\n",
    "12. Markov Decision Process, reinforcement Learning\n",
    "\n",
    "This book will cover most of these algorithms, but our focus will be more on neural network based models because rigorous theory and predictive models can be established. \n",
    "\n",
    "Machine learning is a very active area of research and development. New models, including the so-called cognitive machine learning models are being studied. Models that make use of both physics laws and datasets have been developed. There are also techniques for manipulating various ML models. This book, however, will not cover those topics.\n",
    "\n",
    "${}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General steps for training machine learning models\n",
    "\n",
    "General steps for training machine learning models is summarized as follows. \n",
    "\n",
    "1. Obtaining the dataset for the problem, by your own means of data generation, or imported from other existing sources, or computer syntheses. \n",
    "2. Clean up the dataset if there are objectively known defaults in it.\n",
    "3. Determine the type of hypothesis for the model. \n",
    "3. Develop or import proper module for the needed algorithm for the problem. The learning ability (number of the learning parameters) of the model and the size of the dataset shall be properly balanced, if possible. Otherwise, considering the use of regularization techniques. \n",
    "3. Randomly initializing the learning parameters, or import some known pre-trained learning parameter.\n",
    "4. Perform the training with proper optimization techniques and monitoring measures.\n",
    "5. Test the trained model using independent test dataset. This can also be done during the training. \n",
    "6. Deploy the trained and tested model to the same type of problems, where the training and testing datasets are collected/generated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some mathematical concepts, variables, and spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall define variables and spaces often used in this book for ease of discussion. We first state that this book deals with only real numbers, unless specified when geometrically closed operations are required. Let us introduce two toy examples.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toy examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Toy Example-1**, Regression: Assume we are to build a machine learning model to predict the quality of fruits. Based on its 3 **features**: size, weight, and roundness (that can easily observe and measure), we aim to establish a machine learning regression model to predict the values of 2 characteristics: sweetness and vitamin-C content (that are difficult to quantify nondestructively), for any given fruit. To build such a model, we made 8,000 measurements to randomly selected fruits from the market and create a **dataset** with 8,000 paired data-points. Each data-point records the values of these 3 features and paired with the values of these 2 characteristics. The values of these 2 characteristics are called **labels**  (ground truth) to the data-point. The dataset is called labeled dataset that can be used systematically to train the machine learning model.\n",
    "\n",
    "**Toy Example-2**, Classification: Assume we are to build a machine learning model to classify the type of fruits. Based on its 3 **features** (size, weight, and roundness). In this case, we want a machine to predict whether any given fluid is apple or orange, so that it can be packaged separately in an automatic manner. To achieve this, we made 8,000 measurements to randomly selected fruits of these two types from the market, and create a **dataset** with 8,000 paired data-points. Each data-point records the values of these 3 features and paired with 2 **labels** (\"ground truth\") of yes-or-no for apple (or yes-or-no for orange). The dataset is also called labeled dataset for model training.  \n",
    "\n",
    "With understanding of these two typical types of examples, it should be easy to extern to many other types of problems to which a machine learning model can be effective. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature space** $\\mathbb{X}^p$: Machine learning uses datasets that contains observed or measured $p$ variables of real numbers $\\mathbb{R}$, often called *features*. In our two toy examples, $p=3$. We may define a $p$-dimensional feature space $\\mathbb{X}^p$ which is a [vector space](https://en.wikipedia.org/wiki/Vector_space) over real numbers in $\\mathbb{R}$ with inner product defined. A vector in $\\mathbb{X}^p$ for an arbitrary point $(x_{1}, x_{2},..., x_{p})$ is written as\n",
    "\n",
    "\\begin{equation} \\label{eqAM1}\n",
    "\\mathbf{x}=[x_{1}, x_{2},..., x_{p}], \\ \\  \\mathbf{x} \\in \\mathbb{X}^p\n",
    "\\end{equation} \n",
    "\n",
    "The origin of $\\mathbb{X}^p$ is at $\\mathbf{x}=[0, 0,..., 0]$ following the standard for all vector spaces. Note we use *italic* for scalar variables, **bold face** for all vectors and matrices, **blackboard bold** for spaces (or sets or of that nature), and this convention is followed throughout this book. Also, we define, in general, all vectors in row vectors, as we usually do in Python programming. A column vector is treated as special case of 2D array (matrix) with only one column. \n",
    "\n",
    "It is clear that the feature space $\\mathbb{X}^p$ is a special (with vector operations defined) case of the real space $\\mathbb{R}^p$. Thus, $\\mathbb{X}^p \\in \\mathbb{R}^p$. \n",
    "\n",
    "Also, $x_{i} (i=1,2,...,p)$ is called linear *basis functions* (not to be confused with the basis vectors for $\\mathbb{X}^p$). Any linear combination of  basis functions $x_{i}$ gives a new $x$ that is still in $\\mathbb{X}^p$. \n",
    "\n",
    "A 2-dimensional feature spaces $\\mathbb{X}^2$ is the black plane $x_1-x_2$ shown in Fig.\\ref{fig:1-X_spaceDef}. \n",
    "\n",
    "<img src=\"./images/1-X_spaceDef.png\" alt=\" \" width=\"380\"/><br>\n",
    "\\begin{figure}\n",
    "  \\includegraphics[width=10cm]{}\n",
    "  \\caption{\\label{fig:1-X_spaceDef} Data-points in a 2D feature space $\\mathbb{X}^{2}$ with blue vectors: $\\mathbf{x}_i=[x_{i1}, x_{i2}]$; and the same data-points in the augmented feature space $\\mathbb{\\overline{X}}^{2}$, called affine space,  with red vectors: $\\mathbf{\\overline{x}}_i=[1, x_{i1}, x_{i2}]$; $i=1,2,3,4$.}\n",
    "\\end{figure}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An measured or observed data-point $\\mathbf{x}_i$ with $p$ features is a discrete point in the space, and the corresponding vector $\\mathbf{x}_i$ is expressed as:\n",
    "\n",
    "\\begin{equation} \\label{eqAM2}\n",
    "\\mathbf{x}_i=[x_{i1}, x_{i2},..., x_{ip}], \\ \\  \\mathbf{x}_i \\in \\mathbb{X}^p, \\forall \\ \\ i=1,2,...,m\n",
    "\\end{equation} \n",
    "\n",
    "where $m$ is the number of measurements or observations or data-points in the dataset. It is also often referred as number of samples in a dataset. For these two toy examples, $m=8,000$. For the example shown in Fig.\\ref{fig:1-X_spaceDef}, these 4 blue vectors are for the four data-points in space $\\mathbb{X}^2$, and $m=4$. \n",
    "\n",
    "These data-points $\\mathbf{x}_i (i=1,2,...,m)$ can be stacked to form a **dataset matrix** $\\mathbf{X} \\in \\mathbb{X}^p$. This is for the convenience in formulation. We do not form such a matrix in computation, because it is usually very huge for big datasets with large $m$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Affine space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Affine space** $\\mathbb{\\overline{X}}^{p}$: It is an **augmented feature space**. It is the red plane shown in Fig.\\ref{fig:1-X_spaceDef}. It has a \"complete\" linear bases (or basis functions):\n",
    "\n",
    "\\begin{equation} \\label{eqLinearBases}\n",
    "\\mathbf{\\overline{x}}=[1, x_{1}, x_{2},..., x_{p}]\n",
    "\\end{equation}\n",
    "\n",
    "The over-line signifies the affine space that is a (hyper-) plane hanging over the feature space. By complete linear basis functions, we mean all bases up to the 1st-order of all the variables including the 0th order. The 0th order basis is the constant basis 1 that provides the **augmentation**. Affine space is not a vector space, because $\\mathbf{0} \\notin \\mathbb{\\overline{X}}^{p}$ and ($\\mathbf{\\overline{x}}_i + \\mathbf{\\overline{x}}_j)  \\notin \\mathbb{\\overline{X}}^{p}$ where $i,j$=1 or 2 or 3 or 4 in Fig.\\ref{fig:1-X_spaceDef}. This special and fundamentally  useful space always has a constant 1 as a component, and thus it does not have an origin by definition. Operation that occurs on an affine space and still stays in an affine space is called affine transformation. It is the most essential operation in major machine learning models, and the **fundamental reason** for such models being **predictive**. \n",
    "\n",
    "An observed data-point with $p$ features can also be presented as an augmented discrete point in the $\\mathbb{\\overline{X}}^{p}$ space and can be expressed by\n",
    "\n",
    "\\begin{equation} \\label{eqAM2+1}\n",
    "\\mathbf{\\overline{x}}_i=[1, x_{i1}, x_{i2},..., x_{ip}], \\ \\  \\mathbf{\\overline{x}}_i \\in \\mathbb{\\overline{X}}^{p}, \\forall \\ \\ i=1,2,...,m\n",
    "\\end{equation} \n",
    "\n",
    "A $\\mathbb{\\overline{X}}^{p}$ space can be created by first spanning  $\\mathbb{X}^{p}$ by one dimension to $\\mathbb{X}^{p+1}$ via introduction of a new variable $x_{0}$ as\n",
    "\n",
    "\\begin{equation} \\label{eq-p&1}\n",
    "[x_{0}, x_{1}, x_{2},..., x_{p}]\n",
    "\\end{equation}\n",
    "\n",
    "and then set $x_{0}=1$. These 4 red vectors shown in Fig.\\ref{fig:1-X_spaceDef} lives in space $\\mathbb{\\overline{X}}^{2}$. \n",
    "\n",
    "Note that the affine space $\\mathbb{\\overline{X}}^{p}$ is neither $\\mathbb{X}^{p+1}$ nor $\\mathbb{X}^{p}$, and is quite special. A vector in a $\\mathbb{\\overline{X}}^{p}$ is in $\\mathbb{X}^{p+1}$ but the tip of the vector is confined in the plane of $x_0=1$. For convenience of discussion in this book, we say that an affine space has a **pseudo-dimension** that is $p+1$. Its true dimension is $p$, but it is a hyperplane in a $\\mathbb{X}^{p+1}$ space.  \n",
    "\n",
    "In terms of function approximation, the linear bases given in Eq.(\\ref{eqLinearBases}) can be used to construct any arbitrary linear function in the feature space.  A *proper* linear combination of these complete linear bases is still in the affine space. Such a combination can be used to perform an **affine transformation**, which will be discussed in detail in Chapter 5. \n",
    "\n",
    "These data-points $\\mathbf{\\overline{x}}_i (i=1,2,...,m)$ are stacked to form an **augmented dataset matrix** $\\mathbf{\\overline{X}} \\in \\mathbb{\\overline{X}}^{p}$, which is the well-known the **moment matrix** in function approximation theory \\cite{liu2013finite, liusmoothed, liu2002mesh, liu2013smoothed}. Again, this is for the convenience in formulation. We may not form such a matrix in computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Label space** $\\mathbb{Y}^k$: Consider a labeled dataset for a supervised machine learning model creation. We shall introduce variables $(y_{1}, y_{2},..., y_{k})$ of real numbers in $\\mathbb{R}$. For toy example-1, $k=2$. We may define a label space $\\mathbb{Y}^k$ over real numbers. It is a vector space. A vector in space $\\mathbb{Y}^k$ can be written as \n",
    "\n",
    "\\begin{equation} \\label{eqAM3}\n",
    "\\mathbf{y}=[y_{1}, y_{2},..., y_{k}], \\ \\  \\mathbf{y} \\in \\mathbb{Y}^k \\in \\mathbb{R}^k\n",
    "\\end{equation} \n",
    "\n",
    "A label in a dataset is paired with a data-point. The label for data-point $\\mathbf{x}_i$ is denoted as $\\mathbf{y}_i$ can be expressed as:\n",
    "\\begin{equation} \\label{eqAM4}\n",
    "\\mathbf{y}_i=[y_{i1}, y_{i2},..., y_{ik}], \\ \\  \\mathbf{y}_i \\in \\mathbb{Y}^k, \\forall \\ \\ i=1,2,...,m\n",
    "\\end{equation} \n",
    "\n",
    "For the toy example-1, $y_{ij} (i=1,2,...,8000, j=1,2) $ are 8000 real numbers in 2D space $\\mathbb{Y}^2$. For the toy example-2, each label, $y_{i1}$ or $y_{i2}$, has a value of $0$ or $1$ (or $-1$ or $1$), but the labels can still be viewed living in $\\mathbb{Y}^2$, but in a confined domain. We often denote this kind of space as $[0, 1]^k$ or $[-1, 1]^k$ for labels with $k$ dimension.  When the values at two ends are not reachable, we then note the spaces as $(0, 1)^k$ or $(-1, 1)^k$. \n",
    "\n",
    "These labels $\\mathbf{y}_i (i=1,2,...,m)$ can be stacked to form a label-set $\\mathbf{Y} \\in \\mathbb{Y}^{k}$, although we may not really do so in computation.\n",
    "\n",
    "Typically, affine transformations end at the output layer in a neural network and produces a vector in a label space, so that a loss function can be constructed there for \"terminal control\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning parameters $\\mathbf{\\hat{w}}$ in a machine learning model are continuous variables that live in a **hypothesis space** noted as $\\mathbb{W}^{P}$ over the real numbers. Learning parameters are also called training or trainable parameters. We use these terms interchangeably. The learning parameters include weights and biases in each and all the layer. The hat above $\\mathbf{w}$ implying that it is a collection of all weights and biases, so that we have single notation in a vector for all learning parameters. Its dimension $P$ depends on type of hypothesis used including the configuration of neural networks or ML models. These parameters always work with feature vectors, resulting in intermediate feature vectors in a new feature space or in a label space, thorough a properly designed architecture. \n",
    "\n",
    "These parameters need to be updated which involves vector operations. To ensure convergence, we would need the vector of all learning parameters obey important vector properties, such as inner products, norms and the Cauchy-Schwartz inequality, etc. We will do such proofs multiple times in this book. Therefore, we require $\\mathbb{W}^{P}$ be a vector space, so that each update to the current learning parameters results  new parameters that are still in the same vector space, until they converge. \n",
    "\n",
    "Note that the learning parameters, in general, are in matrix form or column vectors (that can be viewed as a special case of matrix). In a typical machine learning model, there could be multiple matrices of different sizes. These matrices form **affine transformation matrices** that operates on features on affine spaces. A component in a \"vector\" of the hypothesis space can be in fact a matrix in general, and thus it is not easy to comprehend intuitively. The easiest (and valid) way is to \"flatten\" all the matrix and then \"concatenate\" them together to form a tall vector, and then treat it as a usual vector. We do this kind of flattening and concatenation all the time in Python. Such a flattened tall vector $\\mathbf{\\hat{w}}$ in the hypothesis space $\\mathbb{W}^P$ can be written generally as, \n",
    "\n",
    "\\begin{equation} \\label{eqWP}\n",
    "\\mathbf{\\hat{w}}=[\\mathcal{w}_{0}, \\mathcal{w}_{1},..., \\mathcal{w}_{P}]^\\top \\;\\; \\in \\mathbb{W}^P\n",
    "\\end{equation} \n",
    "\n",
    "We will discuss in later chapters the details about $\\mathbb{W}^{P}$ for various models including calculation of the dimension $P$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of a typical machine learning model, a mathematical view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can define mathematically ML models for prediction as a mapping operator: \n",
    "\n",
    "\\begin{equation} \n",
    "\\mathbb{M}(\\mathbf{\\hat{w}}\\in \\mathbb{W}^{P}; \\mathbf{\\overline{X}}\\in \\mathbb{\\overline{X}}^{p}, \\mathbf{Y}\\in \\mathbb{Y}^{k}): \\mathbb{X}^{p} \\to \\mathbb{Y}^{k}\n",
    "\\label{eqAM5}\n",
    "\\end{equation} \n",
    "\n",
    "It reads that ML model $\\mathbb{M}$ uses a given dataset $\\mathbf{X}$ with $\\mathbf{Y}$ to train its learning parameters $\\mathbf{\\hat{w}}$, and produces a map (or a giant function) that makes a prediction in the label space for any point in the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ML model shown in Eq. (\\ref{eqAM5}) is in fact a **data-parameter converter**: it converts a given dataset to learning parameters during training and then convert the parameters back in making a prediction for a given set of feature variables. It can also be mathematically viewed as a giant function with $k$ components in the feature space $\\mathbb{X}^{p}$ and controlled (parameterized) by the training parameters in $\\mathbb{W}^{P}$. When the parameters are tuned, one gets a set of $k$ giant functions over the feature space. \n",
    "\n",
    "On the other hand, this set of $k$ **giant functions** can also be viewed as continuous (differentiable) functions of these parameters for any given data-point in the dataset, which can be used to form a **loss function** that also **differentiable**. Such a loss function can be the error between these $k$ giant functions and the corresponding $k$ labels given in the dataset. It can be viewed as a **functional** of prediction functions that in turn are functions of $\\mathbf{\\hat{w}}$ in the vector space $\\mathbb{W}^{P}$. The training is to minimize such a loss function for all the data-points in the dataset, by updating the training parameters to become minimizers. This overview picture will be made explicitly in later chapters. The success factors for building a quality ML model include 1) type of hypothesis, 2) number of learning parameters $P$ used, 3) quality (representativeness to the underlaying problem to be modeled, including correctness, size, data-point distribution over the features space, and noise level) of the dataset in $\\mathbb{X}^{p}$, and 4) techniques to find the **minimizer** of learning parameters to best produce the label in the dataset. We will discuss about this in detail in later chapters for different machine learning models. \n",
    "\n",
    "Concepts on spaces are helpful in our later analysis of the predictive properties of machine learning models. Readers may find difficulty to comprehend these concepts at this stage, and thus are advised to just have some rough idea for now and to revisit this section when reading relevant chapters.  Readers may jump to Section 13.1.5 and take a look at Eq.(10) there just for a quick glance on how the spaces evolve in a deepnet. \n",
    "\n",
    "Note also that there are ML models for discontinuous feature variables, and the learning parameters may not need to be continuous. Such methods are often developed based proper intuitive rules and techniques, and we will discuss about quite some of those. The concepts on spaces may not directly applicable, but can often help.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements for creating machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a machine learning model, one would need\n",
    "\n",
    "1. A dataset, which may be obtained via observations, experiments, physics-law-based models. The dataset is usually divided (in a random manner) into two mutually independent subsets: training dataset and testing dataset, typically at a rate of 75:25. The independence of the testing dataset is critical, because ML models are determined largely by the training dataset, and hence their reliability depend on objective testing.  \n",
    "2. Labels with the dataset, if possible. \n",
    "3. Prior information on the dataset if possible, such as the quality of the data, key features of the data. This can be useful in choosing proper algorithm for the problem, and in application of regularization techniques in the training. \n",
    "4. Proper computer software modules and/or effective algorithms.\n",
    "5. A computer and preferably connected to the internet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is the key to any data-based models. There are many types of data are available for different types of problems that one may make use of. For example, \n",
    "     \n",
    "* **Images:** photos from cameras (more often now cellphones), images obtained from the open websites, computer tomography (CT), X-ray, ultrasound, and Magnetic resonance imaging (MRI), etc. \n",
    "* **Computer generated data:** data from proven physics-law-based models, or other surrogate models, or other reliable trained machine learning models, etc.\n",
    "* **Text:** unclassified text documents, books, emails, web-pages, social media records, etc.\n",
    "* **Audio and video:** audio and video recordings.\n",
    "\n",
    "Note that the quality and the sampling domain of the dataset play important role in training reliable machine learning models. Use of a trained model beyond the data sampling domain requires a special caution, because it can go wrong unexpected, and hence very dangerous. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation between physics-law-based and data-based models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning models are in general **slow learner fast predictor**, while physics-law-based models do not need to learn (using existing laws), but slow in prediction. This is because the strategy for physics-law-based models and that for data-based models are quite different. ML models use  datasets to train the parameters, but physics-law-based models use laws to determine the parameter.\n",
    "\n",
    "However, at the detailed computational methodology level, many techniques used in both models are in fact the same or quite similar. For example, when express a variable as a function of other variables, both models use basis functions (polynomial, or radial basis function (RBF), or both). In constructing objective functions, the least squared error formulation is used in both. In addition, the regularization methods used are also quite similar. Therefore, one should not study these models in total isolation. The ideas and techniques may be deeply connected and mutually adaptable. This realization can be useful in better understanding and further development of more effective methods for both models, by exchanging the ideas and techniques from one to another. In general, for physic-law-based computational methods, such as the general form of meshfree methods, we understand reasonably well on why and how a method works in theory \\cite{liu2002mesh}. Therefore, we are quite confident on what we are going to obtain, when a method is used for a problem. For data-based methods, however, this is not always true. Therefore, it is of importance to develop fundamental theories for data-based methods. The author made some attempts \\cite{liu2020NEM} to reveal the relationship between physic-law-based and data-based models, and to establish some theoretical foundation of data-based models. In this book, we will try to discuss the similarities and differences, when a computational method is used in both models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This book offers an introduction to general topics on machine learning. Our focus will be on the basic concepts, fundamental theories and essential computational techniques related to creation of various machine learning models. We decided not to provide a comprehensive document for all the machine learning techniques, models, and algorithms. This is because the topic of machine learning is extremely huge and it is not possible to be comprehensive in contents. Also, it is really not possible for many readers to learn all these contents. In addition, there are in fact plenty of documentations, and codes available publicly online. There is no lack of materials, and there is no need to simply reproduce these materials. In the opinion of the author, the best learning approach is to learn the most essential basics and build a strong foundation, which are sufficient enough to learn other related topics, methods, and algorithms. Most importantly, readers with strong fundamental can even develop innovative and more effective machine models for his/her problems.  \n",
    "\n",
    "Based on this philosophy, the highlights of the book that cannot be found easily or in good completions in the open literature are listed as follows, many of which are the outcomes of author's study in the past years.  \n",
    "\n",
    "1. Detailed discussion and demonstration on predictability for constants and arbitrary linear functions of the basic hypothesis used in major ML models. \n",
    "\n",
    "2. Affine transformation properties and its demonstrations, affine space, affine transformation unit, array, and chained arrays, roles of the weights and biases, roles of activation functions for deepnet construction.\n",
    "\n",
    "3. Examination of predictability of high order functions and a Universal Prediction Theory for deepnets.\n",
    "\n",
    "4. A concept of data-parameter converter, parameter encoding, uniqueness of the encoding. \n",
    "\n",
    "5. Role of affine transformation in SVM, complete description on SVM formulation and the kernel trick. \n",
    "\n",
    "5. Detailed discussion and demonstration on activation functions, Neural-Pulse-Unit (NPU), leading to the Universal Approximation Theorem for wide-nets. \n",
    "\n",
    "6. Differentiation of a function with respect to a vector and matrix, leading to automatic differentiation and Autograd. \n",
    "\n",
    "7. Solution Existence Theory, effects of parallel data-points, predictability of the solution against the label. \n",
    "\n",
    "8. General rule for estimating learning parameters in a deepnet. Morst importanly rules of thumb on relationship between the number of data-points and the number neurons in a neural network (or the total pseudo-dimensions of affine spaces involved).  \n",
    "\n",
    "9. Detailed discussion and demonstration on Tikhonov regularization effects.\n",
    "\n",
    "The author has made substaintial effort to write Python codes to demonstrate the esential and difficult concepts and formulations, which allows readers to comprehent each chapter earlier. Based on the liearning experience of the author, this can make the learning more effective. \n",
    "\n",
    "The chapters of this book are written, in principle, readable independently, by allowing some duplicates. Necessary cross-references between chapters provided are kept minimum.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Who may read this book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The book is written for beginners interested to learn basics on machine learning, including university students who completed their first year, graduate students, researchers, and professionals in engineering and sciences. Engineers and practitioners to learn to building machine learning models will also find the book useful. Basic knowledge on college mathematics is helpful in reading this book smoothly.\n",
    "\n",
    "This book may be used as a textbook for undergraduates (3rd year or senior) and graduate students. If this book is adopted as a textbook, the instructor may contact the author (liugr100@gmail.com) directly for some homeworks and course projects and solutions.  \n",
    "\n",
    "Machine learning is still a faster developing area of research. There still exist many challenging problems, which offers ample opportunities for research to develop new methods and algorithms. Currently it is a hot topic of research and applications. Different techniques are being developed every day, and new businesses are formed constantly. It is the hope of the author that this text can be help in studying existing and developing new machine learning models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codes used in this book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The book is written using Jupiter Notebook with codes. Readers who purchased the book may contact the author directly at liugr100@gmail.com to request a softcopy of the book with codes (which may be updated) for free for academic use, after registration. The condition for use the book and codes developed by the author, in both hardcopy and softcopy, are as follows. \n",
    "\n",
    "1. Users are entirely at their own risk using any of part of the codes and techniques.\n",
    "2. The book and codes is only for your own use. You are not allowed to further distribute, without permission from the author. \n",
    "3. There will be no any user support.\n",
    "4. Proper reference and acknowledgment must be given for the use of the book, codes, ideas, and techniques.\n",
    "\n",
    "Note that the handcrafted codes provided in the book are mainly for studying and better understanding the theory and formulation of ML methods. For productions runs, well-established and well-tested packages should be used, and there are plenty out there, including but not limited to Scikit learn, PyTouch, TensorFlow, and Keras.  Also, our codes provided are often run with various packages/modules. Therefore, care is needed when using these codes, because the behavior of the codes often depends on the versions of Python and all these packages/modules. When the code does not run as expected, version mismatch could be one of the problems. When this book is written, the versions of Python and some of the packages/modules are as follows. \n",
    "\n",
    "* Python 3.6.13 :: Anaconda, Inc.\n",
    "* Jupyter Notebook (web-based) 6.3.0\n",
    "* TensorFlow 2.4.1\n",
    "* keras 2.4.3\n",
    "* gym 0.18.0\n",
    "\n",
    "When issues are encountered running a code, readers may need to check the versions of the packages/modules used. If Anaconda Navigator is used, the versions of all these packages/modules installed with the Python environment is listed when the Python environment is highlighted. You can also check the versions of a package in a code cell of the Jupyter Notebook. For example, to check the version of the current environment of Python, one may use: "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T21:12:31.208778Z",
     "start_time": "2025-01-16T21:12:31.151124Z"
    }
   },
   "source": [
    "!python -V             #! is used to execute an external command"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.13\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the version of a package/module, one may use: \n",
    "+ import package_name\n",
    "+ print('package_name version',package_name)\n",
    "\n",
    "For example, "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T21:12:39.203658Z",
     "start_time": "2025-01-16T21:12:39.192487Z"
    }
   },
   "source": [
    "import keras\n",
    "print('keras version',keras.__version__)\n",
    "import tensorflow as tf\n",
    "print('tensorflow version',tf.version.VERSION)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras version 2.10.0\n",
      "tensorflow version 2.10.0\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the version is indeed an issue, one would need to either modify the code to fit the version, or install the correct version in your system , by may be creating an alternative environment. It is very useful to query on the web using the error message, and solutions or leads can often be found. This is the approach the author often takes most of the time, when having an issue in running a code.  \n",
    "\n",
    "Finally, this book has used materials and information available in the web with links. These links may change over time, because of the nature of the web. The most effective way (and often used by the author) to dealing with this matter is to use keywords to search on-line, if the link is lost.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[<a id=\"cit-liu2013finite\" href=\"#call-liu2013finite\">1</a>] GR Liu and Siu Sin Quek, ``_The finite element method: a practical course_'',  Butterworth-Heinemann, New York, 2013.\n",
    "\n",
    "[<a id=\"cit-liusmoothed\" href=\"#call-liusmoothed\">2</a>] G.R. Liu and TT Nguyen, ``_Smoothed finite element methods_'',  Taylor and Francis Group, NewYork, 2010.\n",
    "\n",
    "[<a id=\"cit-liu2002mesh\" href=\"#call-liu2002mesh\">3</a>] G.R. Liu, ``_Mesh free methods: moving beyond the finite element method_'',  Taylor and Francis Group, New York, 2010.\n",
    "\n",
    "[<a id=\"cit-liu2013smoothed\" href=\"#call-liu2013smoothed\">4</a>] G.R. Liu and Gui-Yong Zhang, ``_Smoothed point interpolation methods: G space theory and weakened weak forms_'',  World Scientific, New Jersey, 2013.\n",
    "\n",
    "[<a id=\"cit-liu592computational\" href=\"#call-liu592computational\">5</a>] G.R. Liu and X Han, ``_Computational inverse techniques in nondestructive evaluation._'',  Taylor and Francis Group, NewYork, 2003.\n",
    "\n",
    "[<a id=\"cit-rosenblatt1962\" href=\"#call-rosenblatt1962\">6</a>] F. Rosenblatt, ``_Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms_'',  1962.  [online](https://books.google.com/books?id=7FhRAAAAMAAJ)\n",
    "\n",
    "[<a id=\"cit-rumelhart:errorprop\" href=\"#call-rumelhart:errorprop\">7</a>] David E. Rumelhart, Geoffrey E. Hinton and Ronald J. Williams, ``_Learning Internal Representations by Error Propagation_'',  1986.\n",
    "\n",
    "[<a id=\"cit-liu2019trumpet\" href=\"#call-liu2019trumpet\">8</a>] Liu G.R., ``_FEA-AI and AI-AI: Two-Way Deepnets for Real-Time Computations for Both Forward and Inverse Mechanics Problems_'', International Journal of Computational Methods, vol. 16, number 08, pp. 1950045,  2019.\n",
    "\n",
    "[<a id=\"cit-liu2021tubenet\" href=\"#call-liu2021tubenet\">9</a>] Liu G.R., Duan SY, Zhang ZM <em>et al.</em>, ``_Tubenet: a special trumpetnet for explicit solutions to inverse problems_'', International Journal of Computational Methods, vol. 18, number 01, pp. 2050030,  2021.  [online](https://doi.org/10.1142/S0219876220500309)\n",
    "\n",
    "[<a id=\"cit-Fukushima1980\" href=\"#call-Fukushima1980\">10</a>] Fukushima Kunihiko, ``_Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position_'', Biological Cybernetics, vol. 36, number 4, pp. 193--202, apr 1980.  [online](https://doi.org/10.1007%2Fbf00344251)\n",
    "\n",
    "[<a id=\"cit-multi-column2012\" href=\"#call-multi-column2012\">11</a>] D. Ciregan, U. Meier and J. Schmidhuber, ``_Multi-column deep neural networks for image classification_'', 2012 IEEE Conference on Computer Vision and Pattern Recognition,  2012.\n",
    "\n",
    "[<a id=\"cit-VALUEVA2020232\" href=\"#call-VALUEVA2020232\">12</a>] Valueva MV., Nagornov NN., Lyakhov PA. <em>et al.</em>, ``_Application of the residue number system to reduce hardware costs of the convolutional neural network implementation_'', Mathematics and Computers in Simulation, vol. 177, number , pp. 232-243,  2020.  [online](https://www.sciencedirect.com/science/article/pii/S0378475420301580)\n",
    "\n",
    "[<a id=\"cit-duanLawnMower2021\" href=\"#call-duanLawnMower2021\">13</a>] Duan Shuyong, Ma Honglei, Liu G.R. <em>et al.</em>, ``_Development of an automatic lawnmower with real-time computer vision for obstacle avoidance_'', International Journal of Computational Methods, vol. , number , pp. Accepted,  2021.\n",
    "\n",
    "[<a id=\"cit-duan2021anchor\" href=\"#call-duan2021anchor\">14</a>] Duan Shuyong, Lu Ningning, Lyu Zhongwei <em>et al.</em>, ``_An anchor box setting technique based on differences between categories for object detection_'', International Journal of Intelligent Robotics and Applications, vol. , number , pp. 1-14,  2021.\n",
    "\n",
    "[<a id=\"cit-mcculloch43a\" href=\"#call-mcculloch43a\">15</a>] Mcculloch Warren and Pitts Walter, ``_A Logical Calculus of Ideas Immanent in Nervous Activity_'', Bulletin of Mathematical Biophysics, vol. 5, number , pp. 127--147,  1943.\n",
    "\n",
    "[<a id=\"cit-Schmidhuber1993\" href=\"#call-Schmidhuber1993\">16</a>] Jürgen Schmidhuber, ``_Habilitation thesis: An ancient experiment with credit assignment across 1200 time steps or virtual layers and unsupervised pre-training for a stack of recurrent NNs_'',  1993, TUM.  [online](https://people.idsia.ch//~juergen/habilitation/node114.html)\n",
    "\n",
    "[<a id=\"cit-Yu2019\" href=\"#call-Yu2019\">17</a>] Yu Yong, Si Xiaosheng, Hu Changhua <em>et al.</em>, ``_A Review of Recurrent Neural Networks: LSTM Cells and Network Architectures_'', Neural Comput, vol. 31, number 7, pp. 1235–1270,  2019.  [online](https://direct.mit.edu/neco/article/31/7/1235/8500/A-Review-of-Recurrent-Neural-Networks-LSTM-Cells)\n",
    "\n",
    "[<a id=\"cit-shi2020tubenet\" href=\"#call-shi2020tubenet\">18</a>] S. Duan, L. Wang, F. Wang <em>et al.</em>, ``_A technique for inversely identifying joint stiffnesses of robot arms via two-way TubeNets_'', ,  2021.\n",
    "\n",
    "[<a id=\"cit-duanUncertaintyInverse2way2021\" href=\"#call-duanUncertaintyInverse2way2021\">19</a>] Duan Shuyong, Shi Lutong, Wang Li <em>et al.</em>, ``_An uncertainty inversion technique using two-way neural network for parameter identification of robot arms_'', Inverse Problems in Science and Engineering, vol. 29, number 13, pp. 3279-3304,  2021.  [online](https://doi.org/10.1080/17415977.2021.1988589)\n",
    "\n",
    "[<a id=\"cit-duanJointStiff2021\" href=\"#call-duanJointStiff2021\">20</a>] Duan Shuyong, Wang Li, Liu G.R. <em>et al.</em>, ``_A technique for inversely identifying joint-stiffnesses of robot arms via two-way TubeNets_'', Inverse Problems in Science & Engineering, vol. , number , pp. Accepted,  2021.\n",
    "\n",
    "[<a id=\"cit-liu2020NEM\" href=\"#call-liu2020NEM\">21</a>] Liu G.R., ``_A Neural Element Method_'', International Journal of Computational Methods, vol. 17, number 07, pp. 2050021,  2020.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "number",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
